# Metrics Summary

This section summarizes the main quantitative results obtained during prototype evaluation. The metrics focus on two key components: transcription quality and conversational analysis performance.

## Transcription Quality

Transcription performance was evaluated using **Word Error Rate (WER)**, comparing automated transcriptions against reference texts produced through manual human transcription.

- **WER (total):** ~24.4%  
- **Average transcription accuracy:** ~75â€“76%

It is important to note that the reference texts were generated by human transcribers. As with any manual process, these references may contain minor inconsistencies, interpretation differences, or transcription errors. Therefore, part of the measured error may reflect natural variability in human transcription rather than purely system inaccuracies.

Despite this, the transcription quality proved sufficient to support downstream conversational analysis tasks.

## Conversational Analysis Performance

The classification of sentiment, emotions, and conversational signals was evaluated by comparing automated outputs with manually reviewed human annotations.

- **Precision:** ~77.9%  
- **F1-score:** ~0.665  

Since the evaluation baseline relies on human judgment, some disagreement between automated and manual classifications is expected. Human interpretation of conversational tone, intent, or emotional signals can vary slightly, meaning that part of the observed differences may stem from subjective annotation variability.

## Interpretation

Overall, the evaluation indicates that the prototype achieves a functional balance between transcription reliability and analytical accuracy. While results are influenced by the inherent variability of human-generated reference data, the system demonstrates consistent performance and produces structured insights suitable for exploratory analysis and decision support.
